{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLOUR = {\n",
    "    'PURPLE':'\\033[95m',\n",
    "    'CYAN':'\\033[96m',\n",
    "    'DARKCYAN':'\\033[36m',\n",
    "    'BLUE':'\\033[94m',\n",
    "    'GREEN':'\\033[92m',\n",
    "    'YELLOW':'\\033[93m',\n",
    "    'RED':'\\033[91m',\n",
    "    'BOLD':'\\033[1m',\n",
    "    'UNDERLINE':'\\033[4m',\n",
    "    'END':'\\033[0m'\n",
    "}\n",
    "\n",
    "def print_bold(*msgs):\n",
    "    print(TEXT_COLOUR['BOLD'])\n",
    "    print(*msgs)\n",
    "    print(TEXT_COLOUR['END'])\n",
    "\n",
    "def print_green(*msgs):\n",
    "    print(TEXT_COLOUR['GREEN'])\n",
    "    print(*msgs)\n",
    "    print(TEXT_COLOUR['END'])\n",
    "\n",
    "def print_error(*msgs):\n",
    "    print(TEXT_COLOUR['RED'])\n",
    "    print(*msgs)\n",
    "    print(TEXT_COLOUR['END'])\n",
    "\n",
    "def wrap_green(msg):\n",
    "    return TEXT_COLOUR['GREEN'] + msg + TEXT_COLOUR['END']\n",
    "\n",
    "def wrap_red(msg):\n",
    "    return TEXT_COLOUR['RED'] + msg + TEXT_COLOUR['END']\n",
    "\n",
    "def up_down_str(val):\n",
    "    msg = str(val)\n",
    "    if val > 0:\n",
    "        msg = wrap_green(msg)\n",
    "    elif val < 0:\n",
    "        msg = wrap_red(msg)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp='xlnet-large'\n",
    "num_layers = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"CoLA\",\"SST-2\",\"MRPC\",\"STS-B\",\"QQP\",\"MNLI\", \"MNLI-MM\", \"QNLI\",\"RTE\"]\n",
    "tasks = [\"CoLA\",\"MRPC\",\"STS-B\",\"QQP\",\"MNLI\", \"MNLI-MM\", \"QNLI\",\"RTE\"]\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    \"CoLA\":[\"mcc\"],\n",
    "    \"MNLI\":[\"acc\"],\n",
    "    \"MNLI-MM\":[\"acc\"],\n",
    "    \"MRPC\":[\"f1\"],\n",
    "    \"QNLI\":[\"acc\"],\n",
    "    \"QQP\":[\"f1\"],\n",
    "    \"RTE\":[\"acc\"],\n",
    "    \"SST-2\":[\"acc\"],\n",
    "    \"STS-B\":[\"spearmanr\"],\n",
    "    \"WNLI\":[\"acc\"] #temp\n",
    "}\n",
    "\n",
    "reported_in_paper = {\n",
    "    \"CoLA\":0.00,\n",
    "    \"MNLI\":0.00,\n",
    "    \"MNLI-MM\":0.0,\n",
    "    \"MRPC\":0.00,\n",
    "    \"QNLI\":0.00,\n",
    "    \"QQP\":0.00,\n",
    "    \"RTE\":0.00,\n",
    "    \"SST-2\":0.00,\n",
    "    \"STS-B\":0.00,\n",
    "    \"WNLI\":0.00\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_average_val(lines):\n",
    "    reported = []\n",
    "    for line in lines:\n",
    "        print('\\t', line)\n",
    "        val = float(line.split('\\t')[1])\n",
    "        if val != 0:\n",
    "            reported.append(val)\n",
    "    out = 0\n",
    "    if len(reported) != 0:\n",
    "        reported.sort(reverse = True)\n",
    "        candidates = [reported[0]]\n",
    "        for j in range(1, len(reported)):\n",
    "            if reported[j] > 0.9 * reported[0]:\n",
    "                candidates.append(reported[j])\n",
    "        out = np.mean(candidates)\n",
    "        \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../exp_results/xlnet-large/CoLA/base-mcc.txt\n",
      "\t 1\t0.5337109702303648\n",
      "\t 5\t0.0\n",
      "\t 3\t0.42141042810372065\n",
      "\t 4\t0.5993174841218436\n",
      "\t 2\t0.0\n",
      "\t 1\t0.0463559874942472\n",
      "\t 2\t0.0\n",
      "\t 3\t-0.020702674026557004\n",
      "\t 4\t-0.020702674026557004\n",
      "23\n",
      "\t 1\t0.0437601222642778\n",
      "\t 2\t-0.020702674026557004\n",
      "\t 3\t-0.020702674026557004\n",
      "\t 4\t0.013712849373137744\n",
      "22\n",
      "\t 1\t0.060190597377967524\n",
      "\t 2\t0.018148342420931135\n",
      "\t 3\t0.03149441738954873\n",
      "\t 4\t0.049848138106847704\n",
      "21\n",
      "\t 1\t0.12845843450656336\n",
      "\t 2\t0.10463172663255679\n",
      "\t 3\t0.0407377624184135\n",
      "\t 4\t0.12845843450656336\n",
      "20\n",
      "\t 1\t0.0641304087886275\n",
      "\t 2\t0.09328267618522347\n",
      "\t 3\t0.14061887120564578\n",
      "\t 4\t0.16188057245149598\n",
      "19\n",
      "\t 1\t0.17858768573654893\n",
      "\t 2\t0.1655181547076954\n",
      "\t 3\t0.1798259675230328\n",
      "\t 4\t0.1638264515073343\n",
      "18\n",
      "\t 1\t0.2084340682295498\n",
      "\t 2\t0.22317703622260507\n",
      "\t 3\t0.22033931623469175\n",
      "\t 4\t0.18552761164714818\n",
      "17\n",
      "\t 1\t0.25502269682328815\n",
      "\t 2\t0.30499605871033847\n",
      "\t 3\t0.21831999836205823\n",
      "\t 4\t0.22891469610262768\n",
      "16\n",
      "\t 1\t0.26758774189465595\n",
      "\t 2\t0.27258313566330106\n",
      "\t 3\t0.2445258947660564\n",
      "\t 4\t0.22788485166375735\n",
      "15\n",
      "\t 1\t0.2593416356162653\n",
      "\t 2\t0.2466630085613332\n",
      "\t 3\t0.24162310210953442\n",
      "\t 4\t0.2788201378911936\n",
      "14\n",
      "\t 1\t0.32083557139153007\n",
      "\t 2\t0.3184146642206088\n",
      "\t 3\t0.3096247270079947\n",
      "\t 4\t0.24661855809018488\n",
      "13\n",
      "\t 1\t0.3636552702802627\n",
      "\t 2\t0.32496185610513273\n",
      "\t 3\t0.32210823392551025\n",
      "\t 4\t0.3599034073903833\n",
      "12\n",
      "\t 1\t\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-f3c92ee7b15a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"../exp_results/{exp}/{task}/{log_file_prefix}-{metric}.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mfine_tuning_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_average_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mlog_file_prefix\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-b631ec975022>\u001b[0m in \u001b[0;36mget_average_val\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mreported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for task in tasks:\n",
    "    task_results = {}\n",
    "    task_metrics = metrics[task]\n",
    "    for metric in task_metrics:\n",
    "        \n",
    "        # base metrics\n",
    "        print(f\"../exp_results/{exp}/{task}/base-{metric}.txt\")\n",
    "        f=open(f\"../exp_results/{exp}/{task}/base-{metric}.txt\", \"r\")\n",
    "        lines = f.read().splitlines()\n",
    "        task_results[f'base-{metric}'] = get_average_val(lines)\n",
    "        \n",
    "        # no layer metrics\n",
    "        \n",
    "        fine_tuning_metrics = []\n",
    "        f=open(f\"../exp_results/{exp}/{task}/no_layer-{metric}.txt\", \"r\")\n",
    "\n",
    "        lines = f.read().splitlines()\n",
    "        fine_tuning_metrics.append(get_average_val(lines))\n",
    "        \n",
    "        # fine-tuned metrics\n",
    "        \n",
    "        log_file_prefix=''\n",
    "        for i in reversed(range(int(num_layers/2), num_layers)):\n",
    "            print(i)\n",
    "            log_file_prefix += str(i)\n",
    "            f=open(f\"../exp_results/{exp}/{task}/{log_file_prefix}-{metric}.txt\", \"r\")\n",
    "            lines = f.read().splitlines()\n",
    "            fine_tuning_metrics.append(get_average_val(lines))\n",
    "            \n",
    "            log_file_prefix +='_'\n",
    "        \n",
    "        task_results[f'{metric}'] = list(reversed(fine_tuning_metrics))\n",
    "        \n",
    "    results[task] = task_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = []\n",
    "\n",
    "for i in range(int(num_layers/2), num_layers):\n",
    "    x_axis.append(str(i))\n",
    "\n",
    "x_axis.append(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(task, y_label, paper, base, reported):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(x_axis, reported)\n",
    "    \n",
    "    plt.xlabel(\"layers\")\n",
    "    plt.ylabel(y_label)\n",
    "    \n",
    "    if paper == 0.0:    \n",
    "        gap = max(reported) - min(reported)\n",
    "        top = max(max(reported), base) + (gap*0.2)\n",
    "        bottom = min(min(reported), base) - (gap*0.2)\n",
    "    \n",
    "        plt.ylim(bottom, top)\n",
    "\n",
    "        plt.axhline(y=base, linestyle='--', c='green')\n",
    "    else:\n",
    "        gap = max(reported) - min(reported)\n",
    "        top = max(max(reported), base, paper) + (gap*0.2)\n",
    "        bottom = min(min(reported), base, paper) - (gap*0.2)\n",
    "    \n",
    "        plt.ylim(bottom, top)\n",
    "\n",
    "        plt.axhline(y=base, linestyle='--', c='green')\n",
    "        plt.axhline(y=paper, linestyle='--', c='red')\n",
    "    \n",
    "    plt.title(f'{exp}-{task} ({round(base,4)})')\n",
    "    plt.savefig(f'images/{exp}/{task}', format='png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for task in tasks:\n",
    "    task_results = results[task]\n",
    "    task_metrics = metrics[task]\n",
    "    for metric in task_metrics:\n",
    "        reported = task_results[metric]\n",
    "        base = task_results[f'base-{metric}']\n",
    "        print_bold(task, metric)\n",
    "        print(f\"\\tbase : {round(base * 100, 2)}\")\n",
    "        print(f\"\\t50% : {round(task_results[metric][0] * 100, 2)}\")\n",
    "        print(f\"\\tnone : {round(task_results[metric][-1] * 100, 2)}\")\n",
    "#         draw_graph(task, metric, reported_in_paper[task], base, reported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
