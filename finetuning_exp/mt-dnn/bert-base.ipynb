{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLOUR = {\n",
    "    'PURPLE':'\\033[95m',\n",
    "    'CYAN':'\\033[96m',\n",
    "    'DARKCYAN':'\\033[36m',\n",
    "    'BLUE':'\\033[94m',\n",
    "    'GREEN':'\\033[92m',\n",
    "    'YELLOW':'\\033[93m',\n",
    "    'RED':'\\033[91m',\n",
    "    'BOLD':'\\033[1m',\n",
    "    'UNDERLINE':'\\033[4m',\n",
    "    'END':'\\033[0m'\n",
    "}\n",
    "\n",
    "def print_bold(*msgs):\n",
    "    print(TEXT_COLOUR['BOLD'])\n",
    "    print(*msgs)\n",
    "    print(TEXT_COLOUR['END'])\n",
    "\n",
    "def print_green(*msgs):\n",
    "    print(TEXT_COLOUR['GREEN'])\n",
    "    print(*msgs)\n",
    "    print(TEXT_COLOUR['END'])\n",
    "\n",
    "def print_error(*msgs):\n",
    "    print(TEXT_COLOUR['RED'])\n",
    "    print(*msgs)\n",
    "    print(TEXT_COLOUR['END'])\n",
    "\n",
    "def wrap_green(msg):\n",
    "    return TEXT_COLOUR['GREEN'] + msg + TEXT_COLOUR['END']\n",
    "\n",
    "def wrap_red(msg):\n",
    "    return TEXT_COLOUR['RED'] + msg + TEXT_COLOUR['END']\n",
    "\n",
    "def up_down_str(val):\n",
    "    msg = str(val)\n",
    "    if val > 0:\n",
    "        msg = wrap_green(msg)\n",
    "    elif val < 0:\n",
    "        msg = wrap_red(msg)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp='bert-base'\n",
    "num_layers = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"CoLA\",\"SST-2\",\"MRPC\",\"STS-B\",\"QQP\",\"MNLI\", \"MNLI-MM\", \"QNLI\", \"RTE\"]\n",
    "\n",
    "metrics = {\n",
    "    \"CoLA\":[\"mcc\"],\n",
    "    \"MNLI\":[\"acc\"],\n",
    "    \"MNLI-MM\":[\"acc\"],\n",
    "    \"MRPC\":[\"f1\"],\n",
    "    \"QNLI\":[\"acc\"],\n",
    "    \"QQP\":[\"f1\"],\n",
    "    \"RTE\":[\"acc\"],\n",
    "    \"SST-2\":[\"acc\"],\n",
    "    \"STS-B\":[\"spearmanr\"],\n",
    "    \"WNLI\":[\"acc\"] #temp\n",
    "}\n",
    "\n",
    "reported_in_paper = {\n",
    "    \"CoLA\":0.00,\n",
    "    \"MNLI\":0.00,\n",
    "    \"MNLI-MM\":0.0,\n",
    "    \"MRPC\":0.00,\n",
    "    \"QNLI\":0.00,\n",
    "    \"QQP\":0.00,\n",
    "    \"RTE\":0.00,\n",
    "    \"SST-2\":0.00,\n",
    "    \"STS-B\":0.00,\n",
    "    \"WNLI\":0.00\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_average_val(lines):\n",
    "    reported = []\n",
    "    for line in lines:\n",
    "        val = float(line.split()[1])\n",
    "        if val != 0:\n",
    "            reported.append(val)\n",
    "    out = 0\n",
    "    if len(reported) != 0:\n",
    "        reported.sort(reverse = True)\n",
    "        candidates = [reported[0]]\n",
    "        for j in range(1, len(reported)):\n",
    "            if reported[j] > 0.9 * reported[0]:\n",
    "                candidates.append(reported[j])\n",
    "        out = np.mean(candidates)\n",
    "        \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../mt_dnn_exp_results/bert-base/CoLA/base-mcc.txt\n",
      "../../mt_dnn_exp_results/bert-base/SST-2/base-acc.txt\n",
      "../../mt_dnn_exp_results/bert-base/MRPC/base-f1.txt\n",
      "../../mt_dnn_exp_results/bert-base/STS-B/base-spearmanr.txt\n",
      "../../mt_dnn_exp_results/bert-base/QQP/base-f1.txt\n",
      "../../mt_dnn_exp_results/bert-base/MNLI/base-acc.txt\n",
      "../../mt_dnn_exp_results/bert-base/MNLI-MM/base-acc.txt\n",
      "../../mt_dnn_exp_results/bert-base/QNLI/base-acc.txt\n",
      "../../mt_dnn_exp_results/bert-base/RTE/base-acc.txt\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for task in tasks:\n",
    "    task_results = {}\n",
    "    task_metrics = metrics[task]\n",
    "    for metric in task_metrics:\n",
    "        \n",
    "        # base metrics\n",
    "        print(f\"../../mt_dnn_exp_results/{exp}/{task}/base-{metric}.txt\")\n",
    "        f=open(f\"../../mt_dnn_exp_results/{exp}/{task}/base-{metric}.txt\", \"r\")\n",
    "        lines = f.read().splitlines()\n",
    "        task_results[f'base-{metric}'] = get_average_val(lines)\n",
    "        \n",
    "        # no layer metrics\n",
    "        \n",
    "        fine_tuning_metrics = []\n",
    "        f=open(f\"../../mt_dnn_exp_results/{exp}/{task}/no_layer-{metric}.txt\", \"r\")\n",
    "\n",
    "        lines = f.read().splitlines()\n",
    "        fine_tuning_metrics.append(get_average_val(lines))\n",
    "        \n",
    "        # fine-tuned metrics\n",
    "        \n",
    "        log_file_prefix=''\n",
    "        for i in reversed(range(int(num_layers/2), num_layers)):\n",
    "            log_file_prefix += str(i)\n",
    "            f=open(f\"../../mt_dnn_exp_results/{exp}/{task}/{log_file_prefix}-{metric}.txt\", \"r\")\n",
    "            lines = f.read().splitlines()\n",
    "            fine_tuning_metrics.append(get_average_val(lines))\n",
    "            \n",
    "            log_file_prefix +='_'\n",
    "        \n",
    "        task_results[f'{metric}'] = list(reversed(fine_tuning_metrics))\n",
    "        \n",
    "    results[task] = task_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = []\n",
    "\n",
    "for i in range(int(num_layers/2), num_layers):\n",
    "    x_axis.append(str(i))\n",
    "\n",
    "x_axis.append(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(task, y_label, paper, base, reported):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(x_axis, reported)\n",
    "    \n",
    "    plt.xlabel(\"layers\")\n",
    "    plt.ylabel(y_label)\n",
    "    \n",
    "    if paper == 0.0:    \n",
    "        gap = max(reported) - min(reported)\n",
    "        top = max(max(reported), base) + (gap*0.2)\n",
    "        bottom = min(min(reported), base) - (gap*0.2)\n",
    "    \n",
    "        plt.ylim(bottom, top)\n",
    "\n",
    "        plt.axhline(y=base, linestyle='--', c='green')\n",
    "    else:\n",
    "        gap = max(reported) - min(reported)\n",
    "        top = max(max(reported), base, paper) + (gap*0.2)\n",
    "        bottom = min(min(reported), base, paper) - (gap*0.2)\n",
    "    \n",
    "        plt.ylim(bottom, top)\n",
    "\n",
    "        plt.axhline(y=base, linestyle='--', c='green')\n",
    "        plt.axhline(y=paper, linestyle='--', c='red')\n",
    "    \n",
    "    plt.title(f'{exp}-{task} ({round(base,4)})')\n",
    "    plt.savefig(f'images/{exp}/{task}', format='png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "CoLA mcc : b - 57.38 h - 55.69 n - 53.64\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "SST-2 acc : b - 92.78 h - 92.55 n - 91.86\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "MRPC f1 : b - 92.28 h - 92.28 n - 85.4\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "STS-B spearmanr : b - 90.88 h - 90.64 n - 89.54\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "QQP f1 : b - 88.38 h - 87.76 n - 87.18\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "MNLI acc : b - 84.44 h - 84.02 n - 76.65\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "MNLI-MM acc : b - 84.67 h - 84.46 n - 77.74\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "QNLI acc : b - 91.11 h - 90.94 n - 85.89\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "RTE acc : b - 78.41 h - 79.49 n - 71.05\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    task_results = results[task]\n",
    "    task_metrics = metrics[task]\n",
    "    for metric in task_metrics:\n",
    "        reported = task_results[metric]\n",
    "        base = task_results[f'base-{metric}']\n",
    "        print_bold(task, metric, ': b -', round(base * 100, 2), 'h -',round(task_results[metric][0] * 100, 2), 'n -', round(task_results[metric][-1] * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['none', '11', '10', '9', '8', '7', '6']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "layer_90  0 layer: none 100.0 %\n",
      "[5, 0, 1, 0, 0, 1, 1, 1, 1]\n",
      "layer_95  5 layer: 7 58.33 %\n",
      "\u001b[1m\n",
      "CoLA 0.573754832442525\n",
      "\u001b[0m\n",
      "\t90 0.5364114280465591 93.49\n",
      "\t95 0.562479932991527 98.03\n",
      "\u001b[1m\n",
      "SST-2 0.9277522935779816\n",
      "\u001b[0m\n",
      "\t90 0.9185779816513762 99.01\n",
      "\t95 0.9259174311926606 99.8\n",
      "\u001b[1m\n",
      "MRPC 0.9227562669317466\n",
      "\u001b[0m\n",
      "\t90 0.8539696446626991 92.55\n",
      "\t95 0.9184687817817112 99.54\n",
      "\u001b[1m\n",
      "STS-B 0.9087928989577281\n",
      "\u001b[0m\n",
      "\t90 0.8954457153626455 98.53\n",
      "\t95 0.9054230987704198 99.63\n",
      "\u001b[1m\n",
      "QQP 0.8837812612880741\n",
      "\u001b[0m\n",
      "\t90 0.8717620837936814 98.64\n",
      "\t95 0.8769679326611739 99.23\n",
      "\u001b[1m\n",
      "MNLI 0.84444218033622\n",
      "\u001b[0m\n",
      "\t90 0.766479877738156 90.77\n",
      "\t95 0.8411003565970454 99.6\n",
      "\u001b[1m\n",
      "MNLI-MM 0.8466639544344995\n",
      "\u001b[0m\n",
      "\t90 0.777379983726607 91.82\n",
      "\t95 0.8429617575264443 99.56\n",
      "\u001b[1m\n",
      "QNLI 0.9111111111111111\n",
      "\u001b[0m\n",
      "\t90 0.8589419732747574 94.27\n",
      "\t95 0.9087314662273476 99.74\n",
      "\u001b[1m\n",
      "RTE 0.7841155234657039\n",
      "\u001b[0m\n",
      "\t90 0.7104693140794224 90.61\n",
      "\t95 0.7913357400722022 100.92\n",
      "\u001b[1m\n",
      "8 94.525 98.2475 3.72\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "\n",
    "layer_90 = []\n",
    "layer_95 = []\n",
    "\n",
    "threshold_90 = 0.9\n",
    "threshold_95 = 0.95\n",
    "x_axis.reverse()\n",
    "\n",
    "for task in tasks:\n",
    "#     print_bold(task)\n",
    "    task_results = results[task]\n",
    "    task_metrics = metrics[task]\n",
    "    for metric in task_metrics:\n",
    "        base = task_results[f'base-{metric}']\n",
    "        reported = copy.deepcopy(task_results[metric])\n",
    "        reported.reverse()\n",
    "        \n",
    "        flag_90 = True\n",
    "        flag_95 = True\n",
    "\n",
    "        for ind, val in enumerate(reported):\n",
    "\n",
    "            if val/base > threshold_90 and flag_90:\n",
    "                flag_90 = False\n",
    "                layer_90.append(ind)\n",
    "                results[task]['90%'] = ind\n",
    "\n",
    "            if val/base > threshold_95 and flag_95:\n",
    "                flag_95 = False\n",
    "                layer_95.append(ind)\n",
    "                results[task]['95%'] = ind\n",
    "\n",
    "        if flag_90:\n",
    "            print(task, \"Fails to achieve 90% threshold\", reported[-1]/base)\n",
    "            layer_90.append(len(reported)-1)\n",
    "            results[task]['90%'] = \"-\"\n",
    "\n",
    "        if flag_95:\n",
    "            print(task, \"Fails to achieve 95% threshold\", reported[-1]/base)\n",
    "            layer_95.append(len(reported)-1)\n",
    "            results[task]['95%'] = \"-\"\n",
    "\n",
    "\n",
    "            \n",
    "print(x_axis)\n",
    "            \n",
    "            \n",
    "print(layer_90)\n",
    "min_layer_ind_90 = max(layer_90)\n",
    "print(\"layer_90 \", min_layer_ind_90, 'layer:', x_axis[min_layer_ind_90], round((1-(min_layer_ind_90/num_layers)) * 100, 2), '%')\n",
    "\n",
    "print(layer_95)\n",
    "min_layer_ind_95 = max(layer_95)\n",
    "print(\"layer_95 \", min_layer_ind_95, 'layer:', x_axis[min_layer_ind_95], round((1-(min_layer_ind_95/num_layers)) * 100, 2), '%')\n",
    "\n",
    "\n",
    "firsts = []\n",
    "seconds = []\n",
    "    \n",
    "for task in tasks:\n",
    "    task_results = results[task]\n",
    "    task_metrics = metrics[task]\n",
    "    for metric in task_metrics:\n",
    "        base = task_results[f'base-{metric}']\n",
    "        reported = copy.deepcopy(task_results[metric])\n",
    "        reported.reverse()\n",
    "        \n",
    "        if task != \"CoLA\":\n",
    "            first = round(100*reported[0]/base, 2)\n",
    "            second = round(100*reported[1]/base, 2)\n",
    "            firsts.append(first)\n",
    "            seconds.append(second)\n",
    "            \n",
    "        print_bold(task, base)\n",
    "        print('\\t90', reported[min_layer_ind_90], round(reported[min_layer_ind_90]/base * 100, 2))\n",
    "        print('\\t95', reported[min_layer_ind_95], round(reported[min_layer_ind_95]/base * 100, 2))\n",
    "        \n",
    "print_bold(len(firsts), np.mean(firsts), np.mean(seconds), round(np.mean(seconds) - np.mean(firsts),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8954457153626455\n",
      "0.8997305503630055\n"
     ]
    }
   ],
   "source": [
    "for task in [\"STS-B\"]:\n",
    "    task_results = results[task]\n",
    "    task_metrics = metrics[task]\n",
    "    for metric in task_metrics:\n",
    "        \n",
    "        print(task_results[metric][-1])\n",
    "        print(task_results[metric][-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_metrics = {\n",
    "    \"CoLA\":\"MCC\",\n",
    "    \"MNLI\":\"Acc.\",\n",
    "    \"MNLI-MM\":\"Acc.\",\n",
    "    \"MRPC\":\"F$_1$\",\n",
    "    \"QNLI\":\"Acc.\",\n",
    "    \"QQP\":\"F$_1$\",\n",
    "    \"RTE\":\"Acc.\",\n",
    "    \"SST-2\":\"Acc.\",\n",
    "    \"STS-B\":\"$\\\\rho$\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{center}\n",
      "\t\\scalebox{0.88}{\n",
      "\t\t\\begin{tabular}{rc|ccccccc} \n",
      "\t\t\\toprule[1pt] \n",
      "\t\t\\multirow{2}{*}{Task (Metric)} & \\multirow{2}{*}{Baseline} & \\multicolumn{7}{c}{Fine-tuned layers} \\\\ \n",
      "\t\t\\cline{3-9} \n",
      "\t\t& & 6-11 & 7-11 & 8-11 & 9-11 & 10-11 & 11-11 & Nsone \\\\ \n",
      "\t\t\t\\midrule\n",
      "\t\t\tCoLA (MCC) & 57.38 & 55.69 & 56.25 & 54.28 & 54.11 & 54.22 & 54.22 & 53.64 \\\\\n",
      "\t\t\tSST-2 (Acc.) & 92.78 & 92.55 & 92.59 & 92.34 & 92.27 & 92.22 & 92.04 & 91.86 \\\\\n",
      "\t\t\tMRPC (F$_1$) & 92.28 & 92.28 & 91.85 & 91.83 & 91.49 & 91.50 & 90.61 & 85.40 \\\\\n",
      "\t\t\tSTS-B ($\\rho$) & 90.88 & 90.64 & 90.54 & 90.47 & 90.30 & 90.15 & 89.97 & 89.54 \\\\\n",
      "\t\t\tQQP (F$_1$) & 88.38 & 87.76 & 87.70 & 87.63 & 87.54 & 87.50 & 87.40 & 87.18 \\\\\n",
      "\t\t\tMNLI (Acc.) & 84.44 & 84.02 & 84.11 & 84.19 & 83.91 & 83.72 & 83.05 & 76.65 \\\\\n",
      "\t\t\tMNLI-mm (Acc.) & 84.67 & 84.46 & 84.30 & 84.25 & 84.16 & 83.90 & 83.34 & 77.74 \\\\\n",
      "\t\t\tQNLI (Acc.) & 91.11 & 90.94 & 90.87 & 90.66 & 90.39 & 90.18 & 89.25 & 85.89 \\\\\n",
      "\t\t\tRTE (Acc.) & 78.41 & 79.49 & 79.13 & 78.48 & 76.82 & 75.60 & 75.23 & 71.05 \\\\\n",
      "\t\t\t\\midrule\\midrule\n",
      "\t\t\tRel. perf. (\\%) & 100.00 & 99.59 & 99.56 & 99.02 & 98.59 & 98.32 & 97.83 & 94.41 \\\\\n",
      "\t\t\\end{tabular}\n",
      "\t}\n",
      "\t\\caption{MTDNN-BERT-base on GLUE}\n",
      "\t\\label{table:finetune-all}\n",
      "\\end{center}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\begin{center}\\n\\t\\\\scalebox{0.88}{\\n\\t\\t\\\\begin{tabular}{rc|ccccccc} \\n\\t\\t\\\\toprule[1pt] \\n\\t\\t\\\\multirow{2}{*}{Task (metric)} & \\\\multirow{2}{*}{Baseline} & \\\\multicolumn{7}{c}{Fine-tuned layers} \\\\\\\\ \\n\\t\\t\\\\cline{3-9} \\n\\t\\t& & 6-11 & 7-11 & 8-11 & 9-11 & 10-11 & 11-11 & None \\\\\\\\ \\n\\t\\t\\t\\\\midrule\")\n",
    "\n",
    "avg_performance = []\n",
    "\n",
    "for task in tasks:\n",
    "    m = metrics[task][0]\n",
    "    base_key = f\"base-{m}\"\n",
    "    \n",
    "    if task == \"MNLI-MM\":\n",
    "        row = f\"\\t\\t\\tMNLI-mm ({latex_metrics[task]}) & \"\n",
    "    else:\n",
    "        row = f\"\\t\\t\\t{task} ({latex_metrics[task]}) & \"\n",
    "    \n",
    "    row += \"{:0.2f}\".format(round(results[task][base_key] * 100, 2))\n",
    "    \n",
    "    for ind, val in enumerate(results[task][m]):\n",
    "        row += \" & {:0.2f}\".format(round(val * 100,2))\n",
    "        \n",
    "        if len(avg_performance) == ind:\n",
    "            avg_performance.append([])\n",
    "            \n",
    "            \n",
    "        percent = (val / results[task][base_key]) * 100\n",
    "        avg_performance[ind].append(percent)\n",
    "        \n",
    "#     row += \"& {}\".format(results[task][\"90%\"])\n",
    "#     row += \"& {}\".format(results[task][\"95%\"])\n",
    "        \n",
    "    row += \" \\\\\\\\\"\n",
    "    print(row)\n",
    "    \n",
    "print(\"\\t\\t\\t\\\\midrule\\\\midrule\")\n",
    "\n",
    "row = \"\\t\\t\\tRel. perf. (\\%) & 100.00\"\n",
    "\n",
    "for perf in avg_performance:\n",
    "    row += \" & {:0.2f}\".format(round(np.mean(perf) ,2))\n",
    "    \n",
    "row += \" \\\\\\\\\"\n",
    "\n",
    "print(row)\n",
    "    \n",
    "print(\"\\t\\t\\\\end{tabular}\\n\\t}\\n\\t\\\\caption{MTDNN-BERT-base on GLUE}\\n\\t\\\\label{table:finetune-all}\\n\\\\end{center}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\begin{tabular}{rc|ccccccc}\n",
    "\t\t\t\\toprule[1pt]\n",
    "\t\t\t\\multirow{2}{*}{Task (Metric)} & \\multirow{2}{*}{Baseline} & \\multicolumn{7}{c}{Fine-tuned layers} \\\\\n",
    "\t\t\t\\cline{3-9}\n",
    "\t\t\t& & 6-11 & 7-11 & 8-11 & 9-11 & 10-11 & 11-11 & none \\\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
